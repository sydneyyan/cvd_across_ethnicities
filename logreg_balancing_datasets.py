# -*- coding: utf-8 -*-
"""logreg_balancing_datasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zPfeJ1X5s2eAyeREsO2jPAPB1nvsVkkZ
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
file_path = '/content/drive/Shareddrives/CS 229 Final Project/cvd-dataset/2022/heart_2022_no_nans.csv'

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import csv
import numpy as np

df=  pd.read_csv(file_path)

df.columns

df[['HadHeartAttack']].value_counts()

df[df['HadHeartAttack'] == "No"]

df_no_sample = df[df['HadHeartAttack'] == "No"].sample(df[['HadHeartAttack']].value_counts()["Yes"])

df_no_sample.shape

df_yes = df[df['HadHeartAttack'] == "Yes"]

df_yes.shape

df_balanced = pd.concat([df_no_sample, df_yes])

df_balanced

from sklearn.model_selection import train_test_split

X = df_balanced.drop(['HadHeartAttack'], axis=1)

X.shape

y = df_balanced[["HadHeartAttack"]]

y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

import statsmodels.formula.api as smf

df_train = pd.concat([X_train, y_train], axis=1)

df_train

# df_train.corrwith(df_train[["y"]])

# df_train['WeightInKilograms'].corr(df_train["y"])

# df_train['physical_activites'].corr(df_train["y"])

df_train["y"] = (df_train["HadHeartAttack"] == "Yes").astype(int)

df_train["y"]

df_train.columns

df_train["AlcoholDrinkers"].value_counts()

df_train["physical_activities"] = (df_train["PhysicalActivities"] == "Yes").astype(int)

df_train["covid_pos"] = (df_train["CovidPos"] == "Yes").astype(int)

df_train["alcohol_drinkers"] = (df_train["AlcoholDrinkers"] == "Yes").astype(int)

df_train.corrwith(df_train['y']).sort_values()

# df_train = df_train.drop(['physical_activites'], axis=1)

df_train['had_diabetes'] = (df_train["HadDiabetes"] == "Yes").astype(int)

df_train.corrwith(df_train['y']).sort_values()

df_train['copd'] = (df_train["HadCOPD"] == "Yes").astype(int)

df_train.corrwith(df_train['y']).sort_values()

df_train['had_angina'] = (df_train["HadAngina"] == "Yes").astype(int)

df_train.corrwith(df_train['y']).sort_values()

df_train['had_stroke'] = (df_train["HadStroke"] == "Yes").astype(int)

df_train.corrwith(df_train['y']).sort_values()

df_train

X = df_train.drop(['y', 'HadHeartAttack'], axis=1)

X

y = df_train['y']

y

train_X, test_X, train_y, test_y = train_test_split(
    X,
    y,
    test_size=.33,
    random_state=42
)

train_X

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(max_iter=120, verbose = 1)
clf.fit(train_X[good_cols], train_y)

test_y_pred = clf.predict(test_X[good_cols])

lg_probabilities = clf.predict_proba(test_X[good_cols])

from sklearn.metrics import log_loss
log_reg_loss = log_loss(test_y, lg_probabilities)
print(log_reg_loss)

r2_score(test_y, test_y_pred)

from sklearn.metrics import f1_score
f1 = f1_score(test_y, test_y_pred > .5)
print(f1)

"""### Confusion matrix"""

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(test_y, test_y_pred)
print(f"Confusion matrix:\n{cm}")

tp = cm[1,1]
fn = cm[1,0]
tn = cm[0,0]
fp = cm[0,1]

accuracy = (tp + tn) / np.sum(cm)
print(f"Accuracy: {accuracy}")

"""## Logistic Regression with Regularization"""

from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}
logreg = LogisticRegression(solver='liblinear', max_iter=500)
grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1', error_score='raise')
print(f"{grid_search}")
grid_search.fit(train_X[good_cols], train_y)

best_logreg = grid_search.best_estimator_
print(best_logreg)

predict = best_logreg.predict(test_X[good_cols])
r2_score(test_y, test_y_pred)

f1 = f1_score(test_y, predict > .5)
print(f"Logistic regression with regularization F1 score: {f1}")

lg_probabilities = best_logreg.predict_proba(test_X[good_cols])

log_reg_loss = log_loss(test_y, lg_probabilities)
print(f"Loss after logistic regression with regularization: {log_reg_loss}")

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_sizes, train_scores, valid_scores = learning_curve(best_logreg, train_X[good_cols], train_y, cv=5, scoring='f1')
plt.plot(train_sizes, train_scores.mean(axis=1), label='Training F1 Score')
plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1 Score')
plt.xlabel('Training Size')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

cm = confusion_matrix(test_y, test_y_pred)
print(f"Confusion matrix:\n{cm}")

tp = cm[1,1]
fn = cm[1,0]
tn = cm[0,0]
fp = cm[0,1]

accuracy = (tp + tn) / np.sum(cm)
print(f"Accuracy: {accuracy}")

"""## Stacking Logistic Regressions"""

from sklearn.linear_model import LogisticRegression

# Train multiple base logistic regression models
logreg1 = LogisticRegression(solver='liblinear', C=0.1)
logreg2 = LogisticRegression(solver='liblinear', C=1.0)
logreg3 = LogisticRegression(solver='liblinear', C=10.0)

logreg1.fit(train_X[good_cols], train_y)
logreg2.fit(train_X[good_cols], train_y)
logreg3.fit(train_X[good_cols], train_y)

pred1 = logreg1.predict(test_X[good_cols])
pred2 = logreg2.predict(test_X[good_cols])
pred3 = logreg3.predict(test_X[good_cols])

stacked_X = np.column_stack((pred1, pred2, pred3))
meta_logreg = LogisticRegression(solver='liblinear')
meta_logreg.fit(stacked_X, test_y)

stacked_pred = meta_logreg.predict(stacked_X)

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_sizes, train_scores, valid_scores = learning_curve(meta_logreg, train_X[good_cols], train_y, cv=5, scoring='f1')
plt.plot(train_sizes, train_scores.mean(axis=1), label='Training F1 Score')
plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1 Score')
plt.xlabel('Training Size')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

lg_probabilities = meta_logreg.predict_proba(stacked_X)

f1 = f1_score(test_y, stacked_pred > .5)
print(f"Logistic regression with regularization F1 score: {f1}")
log_reg_loss = log_loss(test_y, lg_probabilities)
print(f"Loss after stacked logistic regressions: {log_reg_loss}")

cm = confusion_matrix(test_y, stacked_pred)
print(f"Confusion matrix:\n{cm}")

tp = cm[1,1]
fn = cm[1,0]
tn = cm[0,0]
fp = cm[0,1]

accuracy = (tp + tn) / np.sum(cm)
print(f"Accuracy: {accuracy}")

"""# Lightgbm with generalized dataset"""

# !pip install lightgbm

pip install --upgrade lightgbm

import lightgbm as lgb

model = lightgbm.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)

"""## Light GBM performance with rebalanced dataset"""

import numpy as np
from sklearn.model_selection import learning_curve, ShuffleSplit
from sklearn.metrics import make_scorer, f1_score, confusion_matrix

# Define your model
model = lgb.LGBMClassifier(learning_rate=0.1, max_depth=-5, random_state=42)

# Train the model on the entire training set
model.fit(train_X[good_cols], train_y)

prediction = model.predict(test_X[good_cols])

cm = confusion_matrix(test_y, prediction)
print(f"{cm}")

f1 = f1_score(test_y, prediction > .5)
print(f"f1 score for rebalanced dataset = {f1}")

accuracy = (tp + tn) / np.sum(cm)
print(f"Accuracy: {accuracy}")

"""##Light GBM performance with unbalanced dataset"""

# df.replace({'Yes': 1, 'No': 0}, inplace=True)
# df.replace({'Female': 0, 'Male': 1}, inplace=True)
df = pd.read_csv(file_path)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['HadDiabetes']

categorical_cols = ['State', 'Sex', 'GeneralHealth', 'LastCheckupTime', 'RemovedTeeth', 'TetanusLast10Tdap', 'HadDiabetes', 'SmokerStatus', 'ECigaretteUsage', 'RaceEthnicityCategory', 'AgeCategory', 'CovidPos']

for column in df.columns:
  # print(f"Processing column: {column}, Data type: {df[column].dtype}")
  df[column] = encoder.fit_transform(df[column].astype(str))

ub_train, ub_test = train_test_split(df, test_size=0.2)
ub_train.columns
ub_train_X = ub_train.drop(['HadHeartAttack'], axis=1)
ub_test_X = ub_test.drop("HadHeartAttack", axis=1)

ub_train_y = ub_train['HadHeartAttack']
ub_test_y = ub_test['HadHeartAttack']

model_unbalanced = lgb.LGBMClassifier(learning_rate=0.1, max_depth=-5, random_state=42)

# Train the model on the entire training set
model_unbalanced.fit(ub_train_X, ub_train_y)

prediction = model_unbalanced.predict(ub_test_X)

cm = confusion_matrix(ub_test_y, prediction)
print(f"{cm}")

f1 = f1_score(ub_test_y, prediction > .5)
print(f"f1 score for unbalanced dataset = {f1}")

accuracy = (tp + tn) / np.sum(cm)
print(f"Accuracy: {accuracy}")

print(prediction)
print(ub_test_y)
print(len([ub_test_y == p for p in prediction]))
print(len(prediction))