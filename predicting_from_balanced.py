# -*- coding: utf-8 -*-
"""predicting_from_balanced

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jL8c3Bfq56jeaPP7aUg3A-xX9OQN4Wvg
"""

#Loading Data
import pandas as pd
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
file_path = '/content/drive/Shareddrives/CS 229 Final Project/cvd-dataset/2022/heart_2022_no_nans.csv'
import csv
import numpy as np
df=  pd.read_csv(file_path)

#importing sklearn tools for logreg
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#Balancing the dataset
df[['HadHeartAttack']].value_counts()
df[['RaceEthnicityCategory']].value_counts()
df[df['HadHeartAttack'] == "No"]
df_no_sample = df[df['HadHeartAttack'] == "No"].sample(df[['HadHeartAttack']].value_counts()["Yes"])
df_yes = df[df['HadHeartAttack'] == "Yes"]
df_yes[['RaceEthnicityCategory']].value_counts()
#equal numbers CVD and non CVD samples
df_balanced = pd.concat([df_no_sample, df_yes])
#separating labels from dataset
X = df_balanced.drop(['HadHeartAttack'], axis=1)
y = df_balanced[["HadHeartAttack"]]
X_all = df_no_sample.drop(['HadHeartAttack'], axis=1)
y_all = df_no_sample[["HadHeartAttack"]]



import statsmodels.formula.api as smf

#Creating df_train for cross validation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
df_train = pd.concat([X_train, y_train], axis=1)
#Reformatting labels into numbers
df_train["y"] = (df_train["HadHeartAttack"] == "Yes").astype(int)
df_train["covid_pos"] = (df_train["CovidPos"] == "Yes").astype(int)
df_train['had_angina'] = (df_train["HadAngina"] == "Yes").astype(int)
df_train['had_skin_cancer'] = (df_train["HadSkinCancer"] == "Yes").astype(int)
df_train['had_depression'] = (df_train["HadDepressiveDisorder"] == "Yes").astype(int)
df_train['HadKidneyDisease'] = (df_train["HadKidneyDisease"] == "Yes").astype(int)
df_train["physical_activities"] = (df_train["PhysicalActivities"] == "Yes").astype(int)
df_train['had_stroke'] = (df_train["HadStroke"] == "Yes").astype(int)
df_train['HighRiskLastYear'] = (df_train["HighRiskLastYear"] == "Yes").astype(int)
df_train['sex'] = (df_train["Sex"] == "Male").astype(int)
df_train["alcohol_drinkers"] = (df_train["AlcoholDrinkers"] == "Yes").astype(int)
df_train["Black"] = (df_train["RaceEthnicityCategory"] == "Black only, Non-Hispanic").astype(int)
df_train["White"] = (df_train["RaceEthnicityCategory"] == "White only, Non-Hispanic").astype(int)
df_train["Hispanic"] = (df_train["RaceEthnicityCategory"] == "Hispanic").astype(int)
df_train['had_diabetes'] = (df_train["HadDiabetes"] == "Yes").astype(int)
for s in [ 'HadCOPD','DifficultyConcentrating', 'DifficultyWalking', 'DifficultyDressingBathing', 'DifficultyErrands', 'HIVTesting','ChestScan', 'HadAsthma', 'CovidPos']:
       df_train[s] = (df_train[s] == "Yes").astype(int)

X = df_train.drop(['y', 'HadHeartAttack'], axis=1)
y = df_train['y']
train_X, test_X, train_y, test_y = train_test_split(
    X,
    y,
    test_size=.33,
    random_state=42
)

df_train.corrwith(df_train['y']).sort_values()

#Using df_train columns that are the most correlated with CVD
good_cols = [
    "DifficultyConcentrating",
    "DifficultyDressingBathing",
    "DifficultyErrands",
    "sex",
    "HadKidneyDisease",
    "HadCOPD",
    "PhysicalHealthDays",
    "had_stroke",
    "had_diabetes",
    "DifficultyWalking",
    "ChestScan",
    "had_angina",
    "physical_activities",
    "alcohol_drinkers"
]

#Using KNN for predictions
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import make_scorer, f1_score
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(train_X[good_cols], train_y)
knn_predict = knn.predict(test_X[good_cols])
f1_knn = f1_score(knn_predict, test_y)
confusion_knn = confusion_matrix(knn_predict, test_y)

!pip install shap

#Running Random Forest for a balanced dataset
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import learning_curve
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import ShuffleSplit
from sklearn.metrics import make_scorer, f1_score
cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
f1_scorer = make_scorer(f1_score, average='micro')
for i in [100, 125, 150, 175, 200, 225, 250, 275]:
  model = RandomForestClassifier(n_estimators= i, random_state=42)
  train_sizes, train_scores, test_scores = learning_curve(
      model, train_X[good_cols], train_y, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10), scoring=f1_scorer
  )
  # Initialize a SHAP explainer for the selected model
  explainer_rf = shap.TreeExplainer(model)

  # Calculate SHAP values for a subset of data (e.g., the first 100 samples)
  shap_values_rf = explainer_rf.shap_values(train_X[good_cols].iloc[:100, :])

  # Plot SHAP summary plot for the selected features
  shap.summary_plot(shap_values_rf, train_X[good_cols].iloc[:100, :])

  # Plotting Random Forest
  # Calculate mean and standard deviation of training and test scores
  plt.figure(figsize=(10, 6))
  plt.plot(
      train_sizes,
      np.mean(train_scores, axis=1),
      "o-",
      color="r",
      label="Training F1 Score"
  )
  plt.plot(
      train_sizes,
      np.mean(test_scores, axis=1),
      "o-",
      color="g",
      label="Testing F1 Score"
  )

  plt.title(f"Learning Curve with F1 Score and {i} Estimators")
  plt.xlabel("Training examples")
  plt.ylabel("F1 Score")
  plt.legend(loc="best")
  plt.show()
  print(np.mean(test_scores, axis=1)[-1])

#Running logistic regression for a balanced dataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import learning_curve, ShuffleSplit
from sklearn.metrics import make_scorer, f1_score
import pandas as pd

model_logreg = LogisticRegression()

model_logreg.fit(train_X[good_cols], train_y)

cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
f1_scorer = make_scorer(f1_score, average='micro')

train_sizes, train_scores_logreg, test_scores_logreg = learning_curve(
    model_logreg, train_X[good_cols], train_y, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10), scoring=f1_scorer
)

# Access the coefficients after fitting the Logistic Regression model
coefficients = model_logreg.coef_[0]
feature_names = train_X[good_cols].columns

coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})
coef_df = coef_df.sort_values(by='Coefficient', ascending=False)

# Plot mean of training and testing scores
plt.figure(figsize=(10, 6))
plt.plot(
    train_sizes,
    np.mean(train_scores_logreg, axis=1),
    "o-",
    color="r",
    label="Training F1 Score"
)
plt.plot(
    train_sizes,
    np.mean(test_scores_logreg, axis=1),
    "o-",
    color="g",
    label="Testing F1 Score"
)

plt.title("Learning Curve with F1 Score")
plt.xlabel("Training examples")
plt.ylabel("F1 Score")
plt.legend(loc="best")
plt.show()

!pip install lightgbm
!pip install --upgrade lightgbm

#Using Light GBM, generates learning curve and Shapley curves for a balanced dataset
import numpy as np
import matplotlib.pyplot as plt
import lightgbm as lgb
import shap
from sklearn.model_selection import learning_curve, ShuffleSplit
from sklearn.metrics import make_scorer, f1_score

model = lgb.LGBMClassifier(learning_rate=0.1, max_depth=-5, random_state=42)

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42)
f1_scorer = make_scorer(f1_score, average='micro')

train_sizes_gbm, train_scores_gbm, test_scores_gbm = learning_curve(
    model, train_X[good_cols], train_y, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10), scoring=f1_scorer
)

model.fit(train_X[good_cols], train_y)

explainer = shap.TreeExplainer(model)

shap_values = explainer.shap_values(train_X[good_cols].iloc[:100, :])

shap.summary_plot(shap_values, train_X[good_cols].iloc[:100, :])

plt.figure(figsize=(12, 6))
plt.title("Learning Curve")
plt.xlabel("Training Examples")
plt.ylabel("F1 Score")
train_scores_mean = np.mean(train_scores_gbm, axis=1)
test_scores_mean = np.mean(test_scores_gbm, axis=1)
plt.grid()

plt.plot(train_sizes_gbm, train_scores_mean, label="Training Score", color="r")
plt.plot(train_sizes_gbm, test_scores_mean, label="Cross-Validation Score", color="g")

plt.legend(loc="best")
plt.show()

# Utilizing a neural net to make predictions for a balanced dataset
from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(random_state=1, max_iter=500).fit(train_X[good_cols], train_y)
clf.predict_proba(test_X[good_cols])
#output losses here
predictions = clf.predict(test_X[good_cols])
loss_values = clf.loss_curve_
print("Loss Values:", loss_values)
plt.plot(loss_values, marker='o')
plt.title('Loss Curve Over Iterations')
plt.xlabel('Number of Iterations')
plt.ylabel('Loss')
plt.show()

confusion_nn = confusion_matrix(test_y, predictions)
score = clf.score(test_X[good_cols], test_y)

#Generating different datasets for different ethnic groups
from sklearn.linear_model import LinearRegression
def ethnic_subset(train_X, train_y, test_X, test_y, category):
  subset_train = (train_X['RaceEthnicityCategory']==category)
  subset_test = (test_X['RaceEthnicityCategory']==category)
  subset_X = train_X[subset_train]
  subset_y = train_y[subset_train]
  subset_test_X = test_X[subset_test]
  subset_test_y = test_y[subset_test]
  return subset_X, subset_y, subset_test_X, subset_test_y


white = (train_X['RaceEthnicityCategory']=="White only, Non-Hispanic")
white_X = train_X[white]
white_Y = train_y[white]
white_reg = LinearRegression()
white_reg.fit(white_X[good_cols], white_Y)
white_reg.coef_

white_X, white_y, white_test_X, white_test_y = ethnic_subset(train_X, train_y, test_X, test_y, 'White only, Non-Hispanic')
white_reg = LinearRegression()
white_reg.fit(white_X[good_cols], white_Y)
linear_pred = white_reg.predict(white_test_X[good_cols])
f1 = f1_score(white_test_y, linear_pred > .5)

hispanic_X, hispanic_y, hispanic_test_X, hispanic_test_y = ethnic_subset(train_X, train_y, test_X, test_y, 'Hispanic')
hispanic_reg = LinearRegression()
hispanic_reg.fit(hispanic_X[good_cols], hispanic_y)
linear_pred = hispanic_reg.predict(hispanic_test_X[good_cols])
f1 = f1_score(hispanic_test_y, linear_pred > .5)

black_X, black_y, black_test_X, black_test_y = ethnic_subset(train_X, train_y, test_X, test_y, 'Black only, Non-Hispanic')
black_reg = LinearRegression()
black_reg.fit(black_X[good_cols], black_y)
linear_pred = black_reg.predict(black_test_X[good_cols])
f1 = f1_score(black_test_y, linear_pred > .5)